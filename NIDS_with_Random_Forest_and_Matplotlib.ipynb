{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "Rl7umYgpjQ0A",
        "outputId": "22dd0b88-e1c9-46fa-aac6-8cd645d494d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy dataset created successfully.\n",
            "Encoded 'protocol_type' feature.\n",
            "Encoded 'service' feature.\n",
            "Data split into training and testing sets.\n",
            "\n",
            "Training the Random Forest model...\n",
            "Model training complete.\n",
            "\n",
            "--- Model Evaluation ---\n",
            "Accuracy: 0.9967\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       208\n",
            "           1       1.00      0.99      0.99        92\n",
            "\n",
            "    accuracy                           1.00       300\n",
            "   macro avg       1.00      0.99      1.00       300\n",
            "weighted avg       1.00      1.00      1.00       300\n",
            "\n",
            "Plot of feature importances saved as 'feature_importance_plot.png'\n",
            "Trained model and label encoders saved.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import joblib\n",
        "\n",
        "# --- Step 1: Create a self-contained, dummy dataset ---\n",
        "# In a real-world scenario, you would replace this section with a\n",
        "# call to load your actual dataset from a file (e.g., a CSV).\n",
        "def create_dummy_dataset(num_samples=1000):\n",
        "    \"\"\"\n",
        "    Generates a dummy dataset for a simplified Network Intrusion Detection System.\n",
        "    The 'label' is determined by a simple rule on two 'important' features.\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Define features and their relationships\n",
        "    features = {\n",
        "        'duration': np.random.rand(num_samples) * 10,\n",
        "        'src_bytes': np.random.randint(50, 5000, num_samples),\n",
        "        'dst_bytes': np.random.randint(50, 5000, num_samples),\n",
        "        'important_feature_1': np.random.rand(num_samples) * 5,\n",
        "        'important_feature_2': np.random.rand(num_samples) * 3,\n",
        "        'protocol_type': np.random.choice(['tcp', 'udp', 'icmp'], num_samples),\n",
        "        'service': np.random.choice(['http', 'smtp', 'ftp', 'dns'], num_samples),\n",
        "    }\n",
        "    df = pd.DataFrame(features)\n",
        "\n",
        "    # Create a simplified 'label' for intrusion detection (1 = attack, 0 = normal)\n",
        "    # The label is based on a simple rule to ensure the model can learn something.\n",
        "    df['label'] = ((df['important_feature_1'] > 4) | (df['important_feature_2'] > 2.5)).astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "df = create_dummy_dataset()\n",
        "print(\"Dummy dataset created successfully.\")\n",
        "\n",
        "# --- Step 2: Data Preprocessing ---\n",
        "# Separate features (X) and target (y)\n",
        "X = df.drop('label', axis=1)\n",
        "y = df['label']\n",
        "\n",
        "# Identify and encode categorical features using LabelEncoder\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "label_encoders = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col])\n",
        "    label_encoders[col] = le\n",
        "    print(f\"Encoded '{col}' feature.\")\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "print(\"Data split into training and testing sets.\")\n",
        "\n",
        "# --- Step 3: Train the Random Forest model ---\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "print(\"\\nTraining the Random Forest model...\")\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# --- Step 4: Evaluate the model's performance ---\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"\\n--- Model Evaluation ---\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)\n",
        "\n",
        "# --- Step 5: Extract and visualize feature importances ---\n",
        "importances = model.feature_importances_\n",
        "feature_names = X.columns\n",
        "feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
        "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
        "\n",
        "# Plot the feature importances\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.barh(feature_importance_df['feature'], feature_importance_df['importance'], color='teal')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importances from Random Forest Classifier')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importance_plot.png')\n",
        "plt.clf()\n",
        "print(\"Plot of feature importances saved as 'feature_importance_plot.png'\")\n",
        "\n",
        "# --- Step 6: Save the trained model and encoders for future use ---\n",
        "joblib.dump(model, 'nids_model.joblib')\n",
        "joblib.dump(label_encoders, 'label_encoders.joblib')\n",
        "print(\"Trained model and label encoders saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "DlzQf30-jdro",
        "outputId": "4fc856ef-977f-445b-daaf-358511c89432"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy dataset created successfully.\n",
            "\n",
            "Training the Random Forest model...\n",
            "Model training complete.\n",
            "\n",
            "--- Model Evaluation ---\n",
            "Accuracy: 0.9967\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       208\n",
            "           1       1.00      0.99      0.99        92\n",
            "\n",
            "    accuracy                           1.00       300\n",
            "   macro avg       1.00      0.99      1.00       300\n",
            "weighted avg       1.00      1.00      1.00       300\n",
            "\n",
            "Plot of feature importances saved as 'feature_importance_plot.png'\n",
            "\n",
            "Trained model and label encoders saved for future use.\n",
            "\n",
            "--- Corrected Live Detection Example ---\n",
            "Prediction: **Intrusion Detected!** (Label: 1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import joblib\n",
        "\n",
        "# --- Step 1: Create a self-contained, dummy dataset ---\n",
        "def create_dummy_dataset(num_samples=1000):\n",
        "    \"\"\"\n",
        "    Generates a dummy dataset for a simplified Network Intrusion Detection System.\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "\n",
        "    features = {\n",
        "        'duration': np.random.rand(num_samples) * 10,\n",
        "        'src_bytes': np.random.randint(50, 5000, num_samples),\n",
        "        'dst_bytes': np.random.randint(50, 5000, num_samples),\n",
        "        'important_feature_1': np.random.rand(num_samples) * 5,\n",
        "        'important_feature_2': np.random.rand(num_samples) * 3,\n",
        "        'protocol_type': np.random.choice(['tcp', 'udp', 'icmp'], num_samples),\n",
        "        'service': np.random.choice(['http', 'smtp', 'ftp', 'dns'], num_samples),\n",
        "    }\n",
        "    df = pd.DataFrame(features)\n",
        "\n",
        "    df['label'] = ((df['important_feature_1'] > 4) | (df['important_feature_2'] > 2.5)).astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "df = create_dummy_dataset()\n",
        "print(\"Dummy dataset created successfully.\")\n",
        "\n",
        "# --- Step 2: Data Preprocessing and Training ---\n",
        "X = df.drop('label', axis=1)\n",
        "y = df['label']\n",
        "\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "label_encoders = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "print(\"\\nTraining the Random Forest model...\")\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# --- Step 3: Evaluate and Visualize ---\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"\\n--- Model Evaluation ---\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)\n",
        "\n",
        "importances = model.feature_importances_\n",
        "feature_names = X.columns\n",
        "feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
        "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.barh(feature_importance_df['feature'], feature_importance_df['importance'], color='teal')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importances from Random Forest Classifier')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importance_plot.png')\n",
        "plt.clf()\n",
        "print(\"Plot of feature importances saved as 'feature_importance_plot.png'\")\n",
        "\n",
        "# --- Step 4: Corrected live detection example ---\n",
        "# The key here is to make the new data match the training data's structure.\n",
        "# First, you must save the trained model and the encoders to disk.\n",
        "joblib.dump(model, 'nids_model.joblib')\n",
        "joblib.dump(label_encoders, 'label_encoders.joblib')\n",
        "print(\"\\nTrained model and label encoders saved for future use.\")\n",
        "\n",
        "# --- Corrected section for using the model on new data ---\n",
        "print(\"\\n--- Corrected Live Detection Example ---\")\n",
        "# This part simulates loading the model and encoders in a new session\n",
        "loaded_model = joblib.load('nids_model.joblib')\n",
        "loaded_encoders = joblib.load('label_encoders.joblib')\n",
        "\n",
        "# Define a new data point to be classified.\n",
        "# The keys of this dictionary must match the original feature names.\n",
        "new_raw_data = {\n",
        "    'duration': [1.5],\n",
        "    'src_bytes': [2500],\n",
        "    'dst_bytes': [150],\n",
        "    'important_feature_1': [4.5], # This value should trigger an \"attack\" label\n",
        "    'important_feature_2': [0.5],\n",
        "    'protocol_type': ['tcp'],\n",
        "    'service': ['http']\n",
        "}\n",
        "\n",
        "# Convert the new data into a DataFrame\n",
        "new_df = pd.DataFrame(new_raw_data)\n",
        "\n",
        "# Apply the same LabelEncoders as were used during training\n",
        "for col, le in loaded_encoders.items():\n",
        "    if col in new_df.columns:\n",
        "        new_df[col] = le.transform(new_df[col])\n",
        "\n",
        "# Now the feature names and values match the training data.\n",
        "# Make the prediction.\n",
        "prediction = loaded_model.predict(new_df)\n",
        "\n",
        "# Interpret the prediction\n",
        "if prediction[0] == 0:\n",
        "    print(f\"Prediction: Normal traffic (Label: {prediction[0]})\")\n",
        "else:\n",
        "    print(f\"Prediction: **Intrusion Detected!** (Label: {prediction[0]})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHKku3H1kozz",
        "outputId": "76cd3767-ca52-4c2e-e40e-d4a34fdfc0e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and encoders loaded successfully.\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug: * Restarting with watchdog (inotify)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "from flask import Flask, render_template, request\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# --- Model and Encoder Loading ---\n",
        "# IMPORTANT: These files must be in the same directory as this app.py file.\n",
        "try:\n",
        "    model = joblib.load('nids_model.joblib')\n",
        "    label_encoders = joblib.load('label_encoders.joblib')\n",
        "    print(\"Model and encoders loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Required model files ('nids_model.joblib', 'label_encoders.joblib') not found.\")\n",
        "    print(\"Please ensure they are in the same directory as this script.\")\n",
        "    model = None\n",
        "    label_encoders = None\n",
        "\n",
        "# Define the order of features as they were during training\n",
        "FEATURE_ORDER = ['duration', 'src_bytes', 'dst_bytes', 'important_feature_1',\n",
        "                 'important_feature_2', 'protocol_type', 'service']\n",
        "\n",
        "@app.route('/', methods=['GET', 'POST'])\n",
        "def home():\n",
        "    \"\"\"\n",
        "    Handles both GET (displaying the form) and POST (processing form data) requests.\n",
        "    \"\"\"\n",
        "    prediction_result = None\n",
        "\n",
        "    # Check if the model files were loaded successfully\n",
        "    if not model or not label_encoders:\n",
        "        return render_template('index.html', error_message=\"Model files not found. Please check your directory.\")\n",
        "\n",
        "    # Handle POST request when the form is submitted\n",
        "    if request.method == 'POST':\n",
        "        try:\n",
        "            # 1. Get raw data from the form\n",
        "            raw_data = {\n",
        "                'duration': [float(request.form['duration'])],\n",
        "                'src_bytes': [int(request.form['src_bytes'])],\n",
        "                'dst_bytes': [int(request.form['dst_bytes'])],\n",
        "                'important_feature_1': [float(request.form['important_feature_1'])],\n",
        "                'important_feature_2': [float(request.form['important_feature_2'])],\n",
        "                'protocol_type': [request.form['protocol_type']],\n",
        "                'service': [request.form['service']]\n",
        "            }\n",
        "\n",
        "            # 2. Create a DataFrame with the correct feature order\n",
        "            new_df = pd.DataFrame(raw_data, columns=FEATURE_ORDER)\n",
        "\n",
        "            # 3. Apply the same LabelEncoders as were used during training\n",
        "            for col, le in label_encoders.items():\n",
        "                if col in new_df.columns:\n",
        "                    new_df[col] = le.transform(new_df[col])\n",
        "\n",
        "            # 4. Make the prediction\n",
        "            prediction = model.predict(new_df)\n",
        "\n",
        "            # 5. Interpret and store the result\n",
        "            if prediction[0] == 1:\n",
        "                prediction_result = \"Intrusion Detected!\"\n",
        "            else:\n",
        "                prediction_result = \"Normal Traffic\"\n",
        "\n",
        "        except Exception as e:\n",
        "            # Catch any errors during prediction and display a user-friendly message\n",
        "            print(f\"Prediction error: {e}\")\n",
        "            prediction_result = f\"An error occurred during prediction: {e}\"\n",
        "\n",
        "    # Render the HTML page, passing the prediction result\n",
        "    return render_template('index.html', result=prediction_result)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Run the Flask app in debug mode\n",
        "    app.run(debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Use 'model' because that's what you named it in Step 3 of your code\n",
        "joblib.dump(model, 'nids_model.joblib')\n",
        "\n",
        "# Also save the encoders so the Pi knows how to handle 'tcp' and 'http'\n",
        "joblib.dump(label_encoders, 'label_encoders.joblib')\n",
        "\n",
        "print(\"Success! Both files are ready for download.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "W118kyIufjys",
        "outputId": "1c9dd27f-d8bf-4b2e-d6af-8eec7992b5c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-973294282.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Use 'model' because that's what you named it in Step 3 of your code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nids_model.joblib'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Also save the encoders so the Pi knows how to handle 'tcp' and 'http'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}